---
title: "A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs"
collection: publications
permalink: https://proceedings.neurips.cc/paper_files/paper/2023/file/3eceb70f47690051d6769739fbf6294b-Paper-Conference.pdf
excerpt: 'Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.'
date: 2023-12-09
venue: 'NeurIPS'
slidesurl: 'https://neurips.cc/media/neurips-2023/Slides/72690.pdf'
paperurl: 'https://proceedings.neurips.cc/paper_files/paper/2023/file/3eceb70f47690051d6769739fbf6294b-Paper-Conference.pdf'
citation: 'Xingyue Huang, Miguel Romero Orth, İsmail İlkan Ceylan, and Pablo Barceló. A theory of link prediction via relational weisfeiler-leman on knowledge graphs. In NeurIPS, 2023.'
---
<!-- ---
title: "Cooperative graph neural networks"
collection: publications
permalink: https://arxiv.org/abs/2310.01267
excerpt: 'Graph neural networks are popular architectures for graph machine learning, based on iterative computation of node representations of an input graph through a series of invariant transformations. A large class of graph neural networks follow a standard message-passing paradigm: at every layer, each node state is updated based on an aggregate of messages from its neighborhood. In this work, we propose a novel framework for training graph neural networks, where every node is viewed as a player that can choose to either 'listen', 'broadcast', 'listen and broadcast', or to 'isolate'. The standard message propagation scheme can then be viewed as a special case of this framework where every node 'listens and broadcasts' to all neighbors. Our approach offers a more flexible and dynamic message-passing paradigm, where each node can determine its own strategy based on their state, effectively exploring the graph topology while learning. We provide a theoretical analysis of the new message-passing scheme which is further supported by an extensive empirical analysis on a synthetic dataset and on real-world datasets.'
date: 2024-05-01
venue: 'ICML'
slidesurl: 'https://arxiv.org/abs/2310.01267'
paperurl: 'https://arxiv.org/pdf/2310.01267'
citation: 'B Finkelshtein, X Huang, M Bronstein, İİ Ceylan, Cooperative graph neural networks, arXiv preprint arXiv:2310.01267, 2023'
---
 -->
